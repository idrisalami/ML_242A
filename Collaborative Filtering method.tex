
\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\onehalfspacing

\title{Collaborative Filtering Methodology}
\author{Ryan Michael Chekkouri}
\date{December 2025}

\begin{document}

\maketitle

\section{Collaborative Filtering Methodology}

This section describes the methodology used to construct a collaborative filtering system for 
cofounder matching. The system transforms each founder’s attributes into numerical representations, 
computes similarity and complementarity relationships, constructs a combined compatibility matrix, 
and learns latent archetypes through matrix factorization.

\subsection{Data Preprocessing and Feature Engineering}

Each founder’s attributes must first be converted into numerical feature vectors. Features are 
classified into two categories: (1) similarity features, where alignment increases compatibility 
(industry, communication style scores, responsiveness, risk tolerance, execution speed, collaboration 
openness, semantic embeddings from idea text fields), and (2) complementarity features, where 
differences strengthen a founding team (roles, tech\_stack, strengths, weaknesses, preferred roles, 
experience, education level).

Multi-label variables are encoded as multi-hot vectors. Single-label categories are one-hot encoded. 
Numerical attributes are standardized. Free-text fields are embedded using a sentence-transformer 
model to obtain dense semantic representations.

All encoded features are concatenated into a unified matrix $X$. For similarity computations, each 
feature vector is L2-normalized. Based on these feature groups, we construct our own “true” 
compatibility values $R_{ij}$ prior to learning the approximation $\hat{R}_{ij}$ via matrix 
factorization.

\subsection{Similarity and Complementarity Computation}

\subsubsection*{Similarity Matrix}

Similarity is computed across features where alignment matters. Using cosine similarity 
$\text{cos}(x,y) = (x \cdot y)/(\|x\|\|y\|)$ applied to $X_{\text{sim}}$, we compute the similarity 
matrix $S_{ij}$, reflecting alignment in mission, communication style, and idea space.

\subsubsection*{Complementarity Matrix}

Complementarity is computed across features where difference is beneficial. For multi-hot vectors, 
complementarity uses Jaccard distance: $C_{ij} = 1 - J(A,B)$ where $J(A,B)=|A \cap B|/|A \cup B|$. 
Jaccard ignores co-absence and reflects functional non-overlap. Numerical complementarity uses 
standardized distances.

\subsubsection*{Combined Compatibility Matrix}

Similarity and complementarity are combined with equal weight:
\[
R_{ij} = 0.5\,S_{ij} + 0.5\,C_{ij}.
\]

\subsection{Matrix Factorization}

Matrix factorization decomposes $R$ into two matrices $P$ and $Q$, where each founder is represented 
as a “user” vector $P[i]$ and an “item” vector $Q[j]$. Predicted compatibility is:
\[
\hat{R}_{ij} = \mu + b_u[i] + b_i[j] + P[i]\cdot Q[j].
\]

Here, $\mu$ is the global average, $b_u[i]$ measures how founder $i$ tends to evaluate others, 
$b_i[j]$ measures how founder $j$ tends to be evaluated, and $P[i]\cdot Q[j]$ models latent 
interactions.

We minimize:
\[
\sum_{i,j} (R_{ij}-\hat{R}_{ij})^2 + \lambda(\|P\|^2+\|Q\|^2+\|b_u\|^2+\|b_i\|^2),
\]
using Stochastic Gradient Descent (SGD). Regularization $\lambda$ prevents overfitting.

\subsection{Hyperparameter Tuning}

The number of latent archetypes $A$ and regularization strength $\lambda$ are optimized using 
grid search with cross-validation, selecting the configuration minimizing validation RMSE.

\subsection{Recommendation Output}

After training, the model computes $\hat{R}_{ij}$ for all founder pairs. Ranking these scores produces 
personalized cofounder recommendations balancing similarity and complementary skill coverage.

\end{document}
